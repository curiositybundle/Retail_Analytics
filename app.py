# -*- coding: utf-8 -*-
"""Retail Intelligence Optimization (RIO) Dashboard

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yY-T1k60Rx1yXU7I13WVIZmZ2e6XPdqL
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt
from collections import defaultdict

# Importing scikit-learn components
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import RobustScaler, PowerTransformer, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score
import warnings

# Suppress warnings for cleaner UI output
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# --- CONFIGURATION & GLOBAL VARS ---
st.set_page_config(layout="wide", page_title="Retail Intelligence Optimization (RIO) Dashboard", menu_items={
    'About': "# RIO: Advanced Retail Analytics Pipeline.\nBuilt by an AIOps Engineer."
})

CATEGORY_FUNNEL_ORDER = ['discovery', 'consideration', 'cart', 'checkout', 'conversion']
POST_CONVERSION_EVENTS = ['order_confirm', 'email_receipt']
TRANSACTIONAL_TOUCHPOINTS_FOR_EXCLUSION = ['checkout_start', 'payment_info_enter', 'payment_submit', 'payment_confirm']
U_SHAPED_WEIGHTS = {'first': 0.40, 'last': 0.40, 'mid': 0.20}
W_SHAPED_WEIGHTS = {'first': 0.30, 'last': 0.30, 'mid': 0.40}
START = '(start)'
CONVERSION = '(conversion)'
ABANDONMENT = '(abandonment)'
CLUSTERING_SAMPLE_SIZE = 10000 # Max customers to sample for faster K-Means/PCA


# --- DATA LOADING AND PIPELINE HELPERS ---

@st.cache_data
def load_data(uploaded_files):
    """
    Loads and validates all necessary CSV files.
    Returns a dictionary of DataFrames and a success status.
    """
    required_files = {
        'interactions.csv': None,
        'journeys.csv': None,
        'orders.csv': None,
        'products_master.csv': None,
        'reviews.csv': None,
        'customers.csv': None,
        'customer_profiles_extended.csv': None
    }
    loaded_data = {}

    # 1. Load files
    for f in uploaded_files:
        if f.name in required_files:
            try:
                loaded_data[f.name] = pd.read_csv(f)
            except Exception as e:
                return loaded_data, False, f"Error reading {f.name}: {e}"

    # 2. Check for missing required files
    missing_files = [name for name in required_files if name not in loaded_data]
    if missing_files:
        return loaded_data, False, f"Missing required file(s): {', '.join(missing_files)}"

    # 3. Rename keys to match notebook variables
    try:
        data_vars = {
            'interactions_raw': loaded_data['interactions.csv'],
            'journeys': loaded_data['journeys.csv'],
            'orders': loaded_data['orders.csv'],
            'products': loaded_data['products_master.csv'],
            'reviews': loaded_data['reviews.csv'],
            'customers': loaded_data['customers.csv'],
            'customer_profiles': loaded_data['customer_profiles_extended.csv']
        }
    except KeyError as e:
        return loaded_data, False, f"Internal error during key mapping: {e}"

    return data_vars, True, "Data loaded successfully."

@st.cache_data
def prepare_customer_features(interactions_raw, orders, customer_profiles):
    """
    Function to generate RFM, Loyalty and Behavioral Features.
    """
    NOW = pd.Timestamp(dt.datetime(2025, 10, 15))

    interactions_raw['timestamp'] = pd.to_datetime(interactions_raw['timestamp'], errors='coerce')
    orders['order_timestamp'] = pd.to_datetime(orders['order_timestamp'], errors='coerce')

    # Recency
    customer_recency = interactions_raw.groupby('customer_id')['timestamp'].max().reset_index()
    customer_recency['recency'] = (NOW - customer_recency['timestamp']).dt.days
    customer_recency = customer_recency[['customer_id', 'recency']]

    # Frequency
    customer_frequency = interactions_raw.groupby('customer_id')['interaction_id'].count().reset_index()
    customer_frequency.rename(columns={'interaction_id': 'frequency'}, inplace=True)

    # Monetary
    customer_monetary = orders.groupby('customer_id')['payment_value'].sum().reset_index()
    customer_monetary.rename(columns={'payment_value': 'monetary_value'}, inplace=True)

    # Merge RFM base
    rfm = customer_recency.merge(customer_frequency, on='customer_id', how='outer').merge(
        customer_monetary, on='customer_id', how='left')
    rfm.fillna(0, inplace=True)

    # Derived Features
    rfm['monetary_per_interaction'] = rfm.apply(
        lambda row: row['monetary_value'] / row['frequency'] if row['frequency'] > 0 else 0,
        axis=1
    )

    rfm['intensity_ratio'] = rfm.apply(
        lambda row: row['frequency'] / (row['recency'] + 1) if row['recency'] >= 0 else 0,
        axis=1
    )

    interaction_diversity = interactions_raw.groupby('customer_id')['touchpoint_type'].nunique().reset_index()
    interaction_diversity.rename(columns={'touchpoint_type': 'interaction_diversity'}, inplace=True)

    first_interaction = interactions_raw.groupby('customer_id')['timestamp'].min().reset_index()
    first_interaction['time_since_first_interaction'] = (NOW - first_interaction['timestamp']).dt.days
    first_interaction = first_interaction[['customer_id', 'time_since_first_interaction']]

    # Channel Preference: OHE on channel usage percentage
    channel_counts = interactions_raw.groupby(['customer_id', 'channel'])['interaction_id'].count().unstack(fill_value=0)
    sum_axis = channel_counts.sum(axis=1)
    if not sum_axis.empty and (sum_axis != 0).any():
        channel_pct = channel_counts.div(sum_axis.replace(0, np.nan), axis=0).fillna(0).add_prefix('channel_')
    else:
        channel_pct = channel_counts.add_prefix('channel_')
        for col in channel_pct.columns:
            channel_pct[col] = 0.0

    channel_pct.reset_index(inplace=True)

    # Final Merge
    customer_features = rfm.merge(interaction_diversity, on='customer_id', how='left')
    customer_features = customer_features.merge(first_interaction, on='customer_id', how='left')
    customer_features = customer_features.merge(channel_pct, on='customer_id', how='left')
    customer_features.fillna(0, inplace=True)

    # Merge with original customer profiles
    customer_data = customer_profiles.merge(customer_features, on='customer_id', how='left')
    customer_data.fillna(0, inplace=True)

    return customer_data

@st.cache_data(show_spinner="Running K-Means Clustering and PCA...")
def run_clustering_pipeline(customer_data):
    """
    Performs preprocessing (Scaling, PowerTransformation, PCA) and K-Means Clustering.
    """
    # 1. Preprocessing for Clustering
    categorical_features = [col for col in customer_data.columns
                           if customer_data[col].dtype == 'object' and col != 'customer_id']
    numerical_features = [col for col in customer_data.columns
                         if customer_data[col].dtype in ['int64', 'float64'] and col not in ['customer_id', 'cluster']]

    customer_data_clean = customer_data.copy()
    for col in categorical_features:
        customer_data_clean[col] = customer_data_clean[col].astype(str).replace(['nan', '0', '0.0', 'None'], 'Unknown')

    customer_data_clean[numerical_features] = customer_data_clean[numerical_features].replace([np.inf, -np.inf], np.nan).fillna(0)

    customer_ids_clean = customer_data_clean['customer_id'].values
    X_full = customer_data_clean[numerical_features + categorical_features].copy()

    # Define Preprocessing Pipelines
    numerical_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('power_transformer', PowerTransformer(method='yeo-johnson')),
        ('robust_scaler', RobustScaler())
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_pipeline, numerical_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ],
        remainder='passthrough'
    )

    # Apply transformations and PCA
    X_scaled = preprocessor.fit_transform(X_full)

    pca = PCA(n_components=0.95, random_state=42)
    X_pca = pca.fit_transform(X_scaled)

    # Fallback/checking for extremely high dimensionality
    if X_pca.shape[1] > 30 or X_pca.shape[1] == 0:
        n_components = min(30, X_scaled.shape[1] - 1)
        if n_components > 0:
            pca = PCA(n_components=n_components, random_state=42)
            X_pca = pca.fit_transform(X_scaled)
        else:
            return None, None, "PCA failed: insufficient features for dimension reduction (0 or 1 feature).", None

    # 2. Determine Optimal K (Simplified to Max Silhouette Score in Range)
    n_clusters_range = range(2, 6)
    silhouette_scores = []

    for k in n_clusters_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X_pca)

        if k > 1:
            try:
                silhouette_scores.append(silhouette_score(X_pca, labels))
            except ValueError:
                 silhouette_scores.append(-1)
        else:
            silhouette_scores.append(-1)

    # Determine optimal K based on max valid silhouette score
    if max(silhouette_scores) > -1:
        optimal_k = n_clusters_range[np.argmax(silhouette_scores)]
    else:
        optimal_k = 3

    # 3. Fit final K-Means model
    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    labels = kmeans_final.fit_predict(X_pca)

    # 4. Create mapping and merge back
    cluster_mapping = pd.DataFrame({
        'customer_id': customer_ids_clean,
        'cluster': labels
    })

    customer_data_with_clusters = customer_data_clean.merge(
        cluster_mapping,
        on='customer_id',
        how='left'
    )
    customer_data_with_clusters['cluster'] = customer_data_with_clusters['cluster'].fillna(-1).astype(int)

    # Create summary
    numerical_cols = [col for col in customer_data_with_clusters.select_dtypes(include=[np.number]).columns.tolist()
                      if col not in ['cluster', 'last_purchase']]
    cluster_summary = customer_data_with_clusters.groupby('cluster').agg({col: 'mean' for col in numerical_cols}).round(3)
    cluster_sizes = customer_data_with_clusters.groupby('cluster').size()
    cluster_summary.insert(0, 'size', cluster_sizes)

    return customer_data_with_clusters, cluster_summary, None, X_pca


# --- VISUALIZATION HELPERS ---

def plot_cluster_summary(cluster_summary):
    """Generates a Matplotlib radar chart for cluster comparison."""
    features = ['recency', 'frequency', 'monetary_value', 'intensity_ratio', 'brand_loyalty']
    if not all(f in cluster_summary.columns for f in features):
        return plt.figure()

    df = cluster_summary[features].copy()

    # Normalize data (MinMax scaling is better for radar charts)
    df_norm = (df - df.min()) / (df.max() - df.min())

    categories = df_norm.columns.tolist()
    N = len(categories)

    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

    for i, cluster in df_norm.iterrows():
        values = cluster.values.flatten().tolist()
        values += values[:1]

        cluster_size = cluster_summary.loc[i]['size']
        label = f'Cluster {i} (Size: {cluster_size:,})'

        ax.plot(angles, values, linewidth=2, linestyle='solid', label=label, marker='o')
        ax.fill(angles, values, 'b', alpha=0.1)

    # Set x-axis labels
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)

    # FIX: Use set_yticks for PolarAxes
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(["20%", "40%", "60%", "80%", "100%"], color="grey", size=8)

    ax.set_title('Cluster Comparison: Key Behavioral Metrics (Normalized)', size=12, y=1.1)
    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

    plt.tight_layout()
    return fig


# --- FUNNEL AND PRODUCT FUNCTIONS ---

def analyze_overall_funnel(interactions_df, funnel_order):
    """Calculates overall sequential funnel reach and drop-off rate."""
    funnel_data = interactions_df.dropna(subset=['funnel_stage']).copy()

    journey_stages = funnel_data.groupby('journey_id')['funnel_stage'].unique()

    journey_funnel = pd.DataFrame(index=journey_stages.index)
    for stage in funnel_order:
        journey_funnel[stage] = journey_stages.apply(lambda stages: stage in stages)

    total_journeys = journey_funnel[funnel_order[0]].sum()

    stage_counts = {}
    for i, stage in enumerate(funnel_order):
        if i == 0:
            stage_counts[stage] = journey_funnel[stage].sum()
        else:
            prev_stage = funnel_order[i-1]
            stage_counts[stage] = journey_funnel[
                (journey_funnel[prev_stage] == True) & (journey_funnel[stage] == True)
            ].index.size

    funnel_summary_df = pd.DataFrame({
        'Stage': funnel_order,
        'Journeys_Reached': [stage_counts.get(s, 0) for s in funnel_order]
    })

    funnel_summary_df['Journeys_Reached_Shift'] = funnel_summary_df['Journeys_Reached'].shift(1, fill_value=total_journeys)
    funnel_summary_df['Drop_Off_Rate'] = 1 - (funnel_summary_df['Journeys_Reached'] / funnel_summary_df['Journeys_Reached_Shift'])
    funnel_summary_df['Drop_Off_Rate'] = (funnel_summary_df['Drop_Off_Rate'] * 100).round(2)
    funnel_summary_df['Journeys_Reached'] = funnel_summary_df['Journeys_Reached'].astype(int)

    funnel_summary_df['Drop_Off'] = (funnel_summary_df['Journeys_Reached_Shift'] - funnel_summary_df['Journeys_Reached']).astype(int)

    funnel_summary_df.drop(columns=['Journeys_Reached_Shift'], inplace=True)

    max_drop_off_stage = funnel_summary_df.loc[funnel_summary_df['Drop_Off_Rate'].idxmax()]

    return funnel_summary_df[['Stage', 'Journeys_Reached', 'Drop_Off', 'Drop_Off_Rate']], max_drop_off_stage


def calculate_funnel_by_channel(interactions_enriched, funnel_order):
    """Calculates funnel stage completion percentage by channel."""
    funnel_data = interactions_enriched.dropna(subset=['funnel_stage', 'channel']).copy()

    journey_stages = funnel_data.groupby('journey_id')['funnel_stage'].unique()
    journey_funnel = pd.DataFrame(index=journey_stages.index)
    for stage in funnel_order:
        journey_funnel[stage] = journey_stages.apply(lambda stages: stage in stages)

    funnel_channel_df = journey_funnel.merge(
        funnel_data[['journey_id', 'channel']].drop_duplicates(subset=['journey_id']),
        on='journey_id',
        how='inner'
    )

    funnel_by_channel = pd.DataFrame()
    for channel in funnel_channel_df['channel'].unique():
        channel_data = funnel_channel_df[funnel_channel_df['channel'] == channel].copy()

        total_channel_journeys = channel_data[funnel_order[0]].sum()

        channel_reach = {}
        for i, stage in enumerate(funnel_order):
            if i == 0:
                channel_reach[stage] = channel_data[stage].sum()
            else:
                prev_stage = funnel_order[i-1]
                channel_reach[stage] = channel_data[
                    (channel_data[prev_stage] == True) & (channel_data[stage] == True)
                ].index.size

        reach_df = pd.Series(channel_reach).to_frame(channel)
        reach_df = (reach_df / total_channel_journeys) * 100
        funnel_by_channel = pd.concat([funnel_by_channel, reach_df], axis=1)

    return funnel_by_channel.fillna(0).T

def calculate_funnel_by_category(interactions_enriched, funnel_order):
    """Calculates funnel efficiency (sequential CR) by category."""
    funnel_data_categories = interactions_enriched.dropna(subset=['funnel_stage', 'category_product']).copy()

    category_stage_completion = (
        funnel_data_categories
        .groupby(['journey_id', 'category_product'])['funnel_stage']
        .unique()
    )

    category_funnel_df = pd.DataFrame(index=category_stage_completion.index)
    for stage in funnel_order:
        category_funnel_df[stage] = category_stage_completion.apply(lambda stages: stage in stages)

    category_funnel_results = []
    all_categories = category_funnel_df.index.get_level_values('category_product').unique()

    for cat in all_categories:
        df_cat = category_funnel_df.loc[pd.IndexSlice[:, cat], :]
        total_journeys_cat = len(df_cat)

        stage_counts = {}
        for i, stage in enumerate(funnel_order):
            if i == 0:
                stage_counts[stage] = df_cat[stage].sum()
            else:
                prev_stage = funnel_order[i-1]
                stage_counts[stage] = df_cat[
                    (df_cat[prev_stage] == True) & (df_cat[stage] == True)
                ].index.size

        summary = pd.DataFrame({
            'Category': cat,
            'Stage': funnel_order,
            'Journeys_Reached': [stage_counts.get(s, 0) for s in funnel_order]
        })

        summary['Drop_Off_Rate'] = 1 - (summary['Journeys_Reached'] / summary['Journeys_Reached'].shift(1, fill_value=total_journeys_cat))
        summary['Reach_Rate'] = 1 - summary['Drop_Off_Rate']

        category_funnel_results.append(summary)

    return pd.concat(category_funnel_results).dropna().reset_index(drop=True).round(4)


def get_product_performance_matrix(interactions_enriched, products):
    """Calculates product conversion rate, views, and review score by channel/device."""
    product_interactions = interactions_enriched[interactions_enriched['product_id'].notna()].copy()

    # Aggregating Performance metrics (Views, Purchases, CR, Reviews)
    product_matrix = (
        product_interactions
        .groupby(['category_product', 'product_id', 'channel', 'device_type'])
        .agg(
            Total_Views=('interaction_id', 'count'),
            Purchases=('is_converted', 'sum'),
            Avg_Review_Score=('avg_review_score', 'mean')
        )
        .reset_index()
    )

    product_matrix['Conversion_Rate'] = product_matrix['Purchases'] / product_matrix['Total_Views']

    product_matrix['Avg_Review_Score'] = product_matrix.groupby('product_id')['Avg_Review_Score'].transform('max')

    product_matrix.fillna(0, inplace=True)
    return product_matrix

def calculate_product_review_summary(reviews):
    """
    Calculates the mean review score and count per product.
    This function uses the 'reviews.csv' file.
    """
    if reviews.empty or 'review_score' not in reviews.columns:
         return pd.DataFrame({'product_id': [], 'Mean_Review_Score': [], 'Review_Count': []})

    try:
        reviews_summary = reviews.groupby('product_id').agg(
            Mean_Review_Score=('review_score', 'mean'),
            Review_Count=('review_id', 'count')
        ).reset_index().round(2)

        return reviews_summary
    except KeyError as e:
        # Catch error if expected column name is missing in uploaded file
        st.error(f"Error processing reviews.csv: Missing column '{e}'. Please check file format.")
        return pd.DataFrame({'product_id': [], 'Mean_Review_Score': [], 'Review_Count': []})


# --- ATTRIBUTION MODELING FUNCTIONS ---

def calculate_single_touch_attribution(df, model_name, position, path_column='channel_sequence'):
    """Calculates First/Last Touch attribution based on sequences."""
    if position == 'first':
        df['touch'] = df[path_column].apply(lambda x: x[0] if len(x) > 0 else 'Unknown')
    elif position == 'last':
        df['touch'] = df[path_column].apply(lambda x: x[-1] if len(x) > 0 else 'Unknown')

    score = df.groupby('touch')['conversion_value'].sum().reset_index(name=f'{model_name}_score')
    total_score = df['conversion_value'].sum()
    if total_score > 0:
        score[f'{model_name}_pct'] = score[f'{model_name}_score'] / total_score
    else:
        score[f'{model_name}_pct'] = 0.0

    return score.rename(columns={'touch': 'key'}).sort_values(f'{model_name}_score', ascending=False)

def calculate_linear_attribution(interactions_df, path_column, journey_sequences_and_value):
    interactions_df = interactions_df[interactions_df['journey_id'].isin(journey_sequences_and_value['journey_id'])].copy()

    if 'conversion_value' not in interactions_df.columns or 'total_touchpoint_count' not in interactions_df.columns:
        return pd.DataFrame({'key': [], 'linear_pct': []})

    interactions_df['linear_credit'] = interactions_df['conversion_value'] / interactions_df['total_touchpoint_count']
    attribution = interactions_df.groupby(path_column)['linear_credit'].sum().reset_index(name='linear_score')

    total_score = attribution['linear_score'].sum()
    if total_score > 0:
        attribution['linear_pct'] = attribution['linear_score'] / total_score
    else:
        attribution['linear_pct'] = 0.0

    return attribution.rename(columns={path_column: 'key'}).sort_values('linear_score', ascending=False)

def get_value_weighted_transitions(df, path_column='channel_sequence'):
    weighted_transitions = []
    for _, row in df.iterrows():
        sequence = row[path_column]
        value = row['conversion_value']
        if not sequence: continue
        weighted_transitions.append({'from_state': START, 'to_state': sequence[0], 'value': value})
        for i in range(len(sequence) - 1):
            weighted_transitions.append({'from_state': sequence[i], 'to_state': sequence[i+1], 'value': value})
        weighted_transitions.append({'from_state': sequence[-1], 'to_state': CONVERSION, 'value': value})
    return pd.DataFrame(weighted_transitions)

def calculate_markov_attribution(df, all_states, path_column='channel_sequence'):
    """
    Simplified Markov model proxy using proportional value distribution
    (for speed) based on the total value of each channel's appearance.
    """
    all_value_transitions = get_value_weighted_transitions(df, path_column)

    if all_value_transitions.empty:
        return pd.DataFrame({'key': all_states, 'markov_pct': 0.0, 'removal_effect': 0.0})

    # Step 1: Calculate total value passing through each state
    state_value_sum = all_value_transitions.groupby('from_state')['value'].sum()

    # Step 2: Total conversion value for normalization
    total_conversion_value = df['conversion_value'].sum()

    # Step 3: Distribute total conversion value proportional to state value (Simplified Score)
    markov_scores = {}

    for state in all_states:
        # Use the ratio of value flowing through the state to the total value
        # as a proxy for the Markov credit.
        markov_scores[state] = state_value_sum.get(state, 0.0)

    # Create the attribution DataFrame
    attribution_df = pd.Series(markov_scores).reset_index(name='markov_score').rename(columns={'index': 'key'})

    if attribution_df['markov_score'].sum() > 0:
        attribution_df['markov_pct'] = attribution_df['markov_score'] / attribution_df['markov_score'].sum()
    else:
        attribution_df['markov_pct'] = 0.0

    # Removal Effect is simplified to 1.0 (or a high number) since we skip the matrix logic
    removal_df = pd.DataFrame({'key': all_states, 'removal_effect': 1.0})

    return attribution_df.merge(removal_df, on='key', how='right').fillna(0).sort_values('markov_score', ascending=False)


def calculate_touchpoint_attribution_models(converted_journeys, interactions_enriched):
    """
    Calculates various attribution models (Linear, Markov, First/Last Touch)
    for both channels and touchpoints.
    """

    # --- Data Prep for Attribution ---

    path_data_filtered = converted_journeys[
        ~converted_journeys['touchpoint_type'].isin(TRANSACTIONAL_TOUCHPOINTS_FOR_EXCLUSION)
    ].copy()

    journey_sequences_and_value = (
        path_data_filtered
        .sort_values(by=['journey_id', 'timestamp'])
        .groupby('journey_id')
        .agg(
            touchpoint_sequence=('touchpoint_type', lambda x: list(x)),
            channel_sequence=('channel', lambda x: list(x)),
            conversion_value=('conversion_value', 'max'),
            cluster=('cluster', 'first')
        )
        .reset_index()
    )

    # Filter for Multi-Touch Journeys only
    journey_sequences_and_value = journey_sequences_and_value[
        journey_sequences_and_value['channel_sequence'].apply(len) > 1
    ].copy()

    all_channels = converted_journeys['channel'].unique().tolist()
    all_touchpoints = converted_journeys['touchpoint_type'].unique().tolist()

    # --- Channel Attribution ---
    ft_channel = calculate_single_touch_attribution(journey_sequences_and_value.copy(), 'first_touch', 'first', 'channel_sequence')
    lt_channel = calculate_single_touch_attribution(journey_sequences_and_value.copy(), 'last_touch', 'last', 'channel_sequence')
    linear_channel = calculate_linear_attribution(path_data_filtered.copy(), 'channel', journey_sequences_and_value)
    markov_channel = calculate_markov_attribution(journey_sequences_and_value.copy(), all_channels, path_column='channel_sequence')

    channel_models = [
        ft_channel.rename(columns={'first_touch_pct': 'First_Touch_pct'}),
        lt_channel.rename(columns={'last_touch_pct': 'Last_Touch_pct'}),
        linear_channel.rename(columns={'linear_pct': 'Linear_pct'}),
        markov_channel.rename(columns={'markov_pct': 'Markov_pct', 'removal_effect': 'Removal_Effect'}),
    ]

    # Merge all channel models
    base_index_ch = pd.DataFrame({'key': all_channels}).set_index('key')
    final_channel_summary = base_index_ch.copy()

    for df in channel_models:
        df_to_merge = df.drop(columns=[col for col in df.columns if 'score' in col], errors='ignore').set_index('key')
        final_channel_summary = final_channel_summary.merge(df_to_merge, left_index=True, right_index=True, how='left')

    final_channel_summary = (final_channel_summary * 100).fillna(0).round(2)
    final_channel_summary.index.name = 'Channel'
    final_channel_summary = final_channel_summary.sort_values('Markov_pct', ascending=False)

    # --- Touchpoint Attribution ---
    linear_touchpoint = calculate_linear_attribution(path_data_filtered.copy(), 'touchpoint_type', journey_sequences_and_value)

    touchpoint_models = [
        linear_touchpoint.rename(columns={'linear_pct': 'Linear_pct'})
    ]

    base_index_tp = pd.DataFrame({'key': all_touchpoints}).set_index('key')
    final_touchpoint_summary = base_index_tp.copy()

    for df in touchpoint_models:
        df_to_merge = df.drop(columns=[col for col in df.columns if 'score' in col], errors='ignore').set_index('key')
        final_touchpoint_summary = final_touchpoint_summary.merge(df_to_merge, left_index=True, right_index=True, how='left')

    final_touchpoint_summary = (final_touchpoint_summary * 100).fillna(0).round(2)
    final_touchpoint_summary.index.name = 'Touchpoint'
    final_touchpoint_summary = final_touchpoint_summary.sort_values('Linear_pct', ascending=False)

    return final_channel_summary, final_touchpoint_summary


# --- MAIN EXECUTION FUNCTION ---

@st.cache_data(show_spinner="Running RIO Analytic Pipeline...")
def execute_pipeline(interactions_raw, journeys, orders, products, reviews, customers, customer_profiles):
    """
    Executes the entire RIO pipeline from data preparation to final analysis.
    Returns a dictionary of all final output DataFrames.
    """
    results = {}

    # --- 1. Data Cleaning and Enrichment ---
    interactions = interactions_raw[
        ~interactions_raw['touchpoint_type'].isin(POST_CONVERSION_EVENTS)
    ].copy()

    customer_data = prepare_customer_features(interactions_raw, orders, customer_profiles)

    # Data Type Conversion and Enrichment (Enrichment is essential for Attribution)
    interactions_enriched = interactions.merge(
        orders[['journey_id', 'customer_id', 'product_id', 'order_timestamp', 'payment_value', 'order_id']],
        on=['journey_id', 'customer_id', 'product_id'],
        how='left',
        suffixes=('', '_order')
    )

    interactions_enriched = interactions_enriched.merge(
        products.rename(columns={'category': 'category_product', 'avg_review_score': 'avg_review_score_master'}),
        on='product_id',
        how='left',
        suffixes=('', '_product_master')
    )

    interactions_enriched['is_converted'] = interactions_enriched['order_id'].notna()
    interactions_enriched['timestamp'] = pd.to_datetime(interactions_enriched['timestamp'], errors='coerce')
    interactions_enriched['avg_review_score'] = interactions_enriched['avg_review_score_master']

    # Funnel Stage Mapping
    funnel_stage_map = {
        "search": "discovery", "social_media": "discovery", "referral": "discovery", "email_click": "discovery",
        "product_view": "consideration", "compare_product": "consideration", "review_read": "consideration", "video_watch": "consideration",
        "cart_add": "cart", "wishlist_add": "cart", "cart_view": "cart",
        "checkout_start": "checkout", "payment_info_enter": "checkout", "payment_submit": "checkout",
        "payment_confirm": "conversion", "order_confirm": "conversion", "email_receipt": "conversion"
    }
    interactions_enriched['funnel_stage'] = interactions_enriched['touchpoint_type'].map(funnel_stage_map)


    # --- 2. Customer Segmentation ---
    # Sampling for performance
    if len(customer_data) > CLUSTERING_SAMPLE_SIZE:
        customer_data_sampled = customer_data.sample(CLUSTERING_SAMPLE_SIZE, random_state=42).copy()
    else:
        customer_data_sampled = customer_data.copy()

    customer_data_clustered, cluster_summary, pca_error, X_pca = run_clustering_pipeline(customer_data_sampled)

    if customer_data_clustered is not None and 'cluster' in customer_data_clustered.columns:
        cluster_mapping = customer_data_clustered[['customer_id', 'cluster']]
        interactions_enriched = interactions_enriched.merge(
            cluster_mapping,
            on='customer_id',
            how='left'
        )
        interactions_enriched['cluster'] = interactions_enriched['cluster'].fillna(-1).astype(int)
    else:
        cluster_summary = pd.DataFrame()
        interactions_enriched['cluster'] = -1

    results['cluster_summary'] = cluster_summary

    # --- 3. Funnel and Product Analysis ---
    funnel_overall, max_drop_off = analyze_overall_funnel(interactions_enriched, CATEGORY_FUNNEL_ORDER)
    results['funnel_overall'] = funnel_overall
    results['funnel_channel_matrix'] = calculate_funnel_by_channel(interactions_enriched, CATEGORY_FUNNEL_ORDER)

    product_matrix = get_product_performance_matrix(interactions_enriched, products)
    results['product_matrix'] = product_matrix

    category_channel_matrix = product_matrix.groupby(['category_product', 'channel'])['Conversion_Rate'].mean().unstack().fillna(0)
    results['category_channel_matrix'] = category_channel_matrix.reset_index()

    results['funnel_category_matrix'] = calculate_funnel_by_category(interactions_enriched, CATEGORY_FUNNEL_ORDER)
    results['product_reviews'] = calculate_product_review_summary(reviews)


    # --- 4. Attribution Analysis ---
    converted_journeys = interactions_enriched[interactions_enriched['is_converted'] == True].copy()

    # Merge additional metrics required by attribution functions
    journey_value = converted_journeys.groupby('journey_id')['payment_value'].max().reset_index(name='conversion_value')
    touchpoint_counts = converted_journeys.groupby('journey_id').size().reset_index(name='total_touchpoint_count')
    final_time = converted_journeys.groupby('journey_id')['timestamp'].max().reset_index(name='final_timestamp')

    converted_journeys = converted_journeys.merge(journey_value, on='journey_id', how='left')
    converted_journeys = converted_journeys.merge(touchpoint_counts, on='journey_id', how='left')
    converted_journeys = converted_journeys.merge(final_time, on='journey_id', how='left')
    converted_journeys.dropna(subset=['conversion_value', 'total_touchpoint_count'], inplace=True)
    converted_journeys = converted_journeys.sort_values(by=['journey_id', 'timestamp'])

    final_channel_summary, final_touchpoint_summary = calculate_touchpoint_attribution_models(converted_journeys, interactions_enriched)
    results['final_channel_summary'] = final_channel_summary
    results['final_touchpoint_summary'] = final_touchpoint_summary

    return results

# --- DASHBOARD LAYOUT AND UI ---

def main_app():
    st.title("ðŸ›’ Retail Intelligence Optimization (RIO) Dashboard")
    st.markdown("---")

    # --- File Upload Section (Column Layout) ---
    st.header("1. Data Input")
    st.info("Upload all 7 required CSV files to run the full analytics pipeline.")

    # List required file names for the user
    required_file_names = ['interactions.csv', 'journeys.csv', 'orders.csv', 'products_master.csv', 'reviews.csv', 'customers.csv', 'customer_profiles_extended.csv']
    uploaded_files = st.file_uploader(
        "Upload Required Data Files (7 CSVs)",
        type=['csv'],
        accept_multiple_files=True,
        key="file_uploader"
    )

    data_vars = {}

    if uploaded_files and len(uploaded_files) == len(required_file_names):
        data_vars, success, message = load_data(uploaded_files)

        if success:
            st.success(message)
            # Check if execution has been performed previously in this session
            if 'results' not in st.session_state:
                run_button = st.button("ðŸš€ Run RIO Analysis", key="run_analysis", type='primary')
            else:
                run_button = st.button("ðŸ”„ Rerun RIO Analysis", key="rerun_analysis", type='primary')
        else:
            st.error(message)
            run_button = False
    else:
        st.warning(f"Please upload exactly {len(required_file_names)} files: {', '.join(required_file_names)}")
        run_button = False

    st.markdown("---")

    # --- Execute and Display ---

    if run_button and data_vars:
        # Clear cache and rerun the pipeline
        st.cache_data.clear()
        results = execute_pipeline(**data_vars)
        st.session_state['results'] = results

    if 'results' in st.session_state:
        results = st.session_state['results']

        st.header("2. Analysis Dashboard")

        # --- ROW 1: CLUSTER INSIGHTS ---
        st.subheader("Customer Segmentation Insights")

        col1, col2 = st.columns([1, 1])

        # Formatting dictionary for percentage and integer columns
        percent_format = {col: '{:.2%}' for col in results['final_channel_summary'].select_dtypes(include=np.number).columns if 'pct' in col or 'Effect' in col}
        int_format = {col: '{:,}' for col in results['cluster_summary'].select_dtypes(include=np.integer).columns}

        with col1:
            st.markdown("#### Cluster Descriptive Summary")
            # Combine formatting dicts
            cluster_format = {'size': '{:,}', 'recency': '{:.1f}', 'frequency': '{:.1f}', 'monetary_value': '${:.2f}', 'monetary_per_interaction': '${:.2f}', 'intensity_ratio': '{:.2f}'}
            st.dataframe(
                results['cluster_summary'].reset_index(names=['Cluster ID']).style.background_gradient(cmap='Blues'),
                width='stretch'
            )

        with col2:
            st.markdown("#### RFM & Behavioral Feature Comparison (Radar Plot)")
            try:
                fig = plot_cluster_summary(results['cluster_summary'])
                st.pyplot(fig)
            except Exception as e:
                st.error(f"Error generating Cluster Plot: {e}")


        # --- ROW 2: FUNNEL ANALYSIS ---
        st.subheader("Funnel and Drop-Off Analysis")

        col3, col4 = st.columns([1, 1])

        with col3:
            st.markdown("#### Overall Funnel Drop-Offs (%)")
            st.dataframe(
                results['funnel_overall'].reset_index(drop=True).style.background_gradient(cmap='Reds', subset=['Drop_Off_Rate']).format({
                    'Drop_Off_Rate': '{:.2f}%',
                    'Journeys_Reached': '{:,}',
                    'Drop_Off': '{:,}'
                }),
                width='stretch'
            )

        with col4:
            st.markdown("#### Funnel Reach Rate by Channel")
            # Apply percentage formatting only to numerical columns
            channel_reach_cols = [col for col in results['funnel_channel_matrix'].columns if results['funnel_channel_matrix'][col].dtype in [np.float64, np.number]]
            channel_reach_format = {col: '{:.2f}%' for col in channel_reach_cols}

            st.dataframe(
                results['funnel_channel_matrix'].style.background_gradient(cmap='viridis').format(channel_reach_format),
                width='stretch'
            )

        # --- ROW 3: ATTRIBUTION AND PERFORMANCE ---
        st.subheader("Attribution and Product Strategy")

        st.markdown("#### Marketing Channel Attribution Comparison")
        col5, col6 = st.columns(2)

        # Identify columns that need percentage formatting (all % columns)
        att_format_cols = [col for col in results['final_channel_summary'].columns if 'pct' in col or 'Effect' in col]
        att_format_dict = {col: '{:.2f}%' for col in att_format_cols}

        with col5:
            st.markdown("Attribution (Channels) - Top 4 Models")
            # FIX: Ensure we only format numerical columns
            st.dataframe(
                results['final_channel_summary'].reset_index().style.background_gradient(cmap='PuBu', subset=['Markov_pct', 'Removal_Effect']).format(att_format_dict),
                width='stretch'
            )

        with col6:
            st.markdown("Granular Touchpoint Attribution (Linear Model)")
            tp_format_cols = [col for col in results['final_touchpoint_summary'].columns if 'pct' in col]
            tp_format_dict = {col: '{:.2f}%' for col in tp_format_cols}

            st.dataframe(
                results['final_touchpoint_summary'].reset_index().style.background_gradient(cmap='Oranges', subset=['Linear_pct']).format(tp_format_dict),
                width='stretch'
            )

        # --- ROW 4: PRODUCT MATRIX & CATEGORY EFFICIENCY ---
        st.subheader("Product and Category Efficiency")

        st.markdown("#### Average Product Conversion Rate by Category & Channel")

        # FIX: The category_channel_matrix needs to have its index handled for styling
        # Set the category_product column as the index for styling the pivot table view
        styled_matrix = results['category_channel_matrix'].set_index('category_product').style.background_gradient(cmap='Greens').format('{:.2%}')

        st.dataframe(
            styled_matrix,
            width='stretch'
        )

        st.markdown("#### Sequential Funnel Efficiency by Product Category")
        st.dataframe(
            results['funnel_category_matrix'].reset_index(drop=True).style.background_gradient(cmap='plasma', subset=['Reach_Rate']).format({
                'Reach_Rate': '{:.2%}',
                'Drop_Off_Rate': '{:.2%}',
                'Journeys_Reached': '{:,}'
            }),
            width='stretch'
        )

        # --- NEW ROW 5: PRODUCT REVIEWS ---
        st.subheader("Product Quality Metrics")
        st.markdown("#### Mean Review Score and Count by Product")
        st.dataframe(
            results['product_reviews'].reset_index(drop=True).style.background_gradient(cmap='YlOrRd', subset=['Mean_Review_Score']).format({
                'Mean_Review_Score': '{:.2f}',
                'Review_Count': '{:,}'
            }),
            width='stretch'
        )

# Execute the main application function
if __name__ == "__main__":
    main_app()